{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KelvinYCH5354/Linear-Regression/blob/main/colab_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9FfatPz6MU3"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsoaNwrZA0ui"
      },
      "source": [
        "**Goal**: predict the PM2.5 concentration for the next 10 hours based on the previous 9 hours' data of 18 features (including PM2.5).\n",
        "\n",
        "If there are any questions, please feel free to email the teaching assistant mailbox: xiang.hao@connect.polyu.hk.\n",
        "\n",
        "_Modified from Lee Hong Yee's tutorials_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7RiAkkjCc6l"
      },
      "source": [
        "## Load `train.csv`\n",
        "\n",
        "The data in train.csv consists of 20 days per month, 24 hours per day, and 18 features per hour for each of the 12 months."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AfNX-hB3kN8",
        "outputId": "eaf867f8-c159-459d-fe16-1d0ab2fb2f20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "# Download `data.zip` and extract it.\n",
        "!gdown '1EDo3Q8TTKKPLU1eWE9ZBAOIJUxIAiS9i' --output data.zip\n",
        "!unzip data.zip\n",
        "\n",
        "data = pd.read_csv('./train.csv', encoding = 'utf-8')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EDo3Q8TTKKPLU1eWE9ZBAOIJUxIAiS9i\n",
            "To: /content/data.zip\n",
            "\r  0% 0.00/166k [00:00<?, ?B/s]\r100% 166k/166k [00:00<00:00, 85.5MB/s]\n",
            "Archive:  data.zip\n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqUdj00pDTpo"
      },
      "source": [
        "## Preprocessing\n",
        "Extract the required values and fill the 'RAINFALL' column with 0 (NR: No Rain).\n",
        "\n",
        "Also, if you want to repeat the execution of this code in Colab, please start from the beginning (rerun the above code) to avoid getting unexpected results. (If you write the code yourself, you may not encounter this issue, but in Colab, repeating this code will keep taking data down. That is, the first time you take data from column 3 and beyond, the second time you take the data from the first time and drop the first three columns, and so on.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIGP7XUYD_Yb"
      },
      "source": [
        "data = data.iloc[:, 3:]\n",
        "data[data == 'NR'] = 0\n",
        "raw_data = data.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7PCrVwX6jBF"
      },
      "source": [
        "## Extract Features (1)\n",
        "\n",
        "![image](https://drive.google.com/uc?id=1LyaqD4ojX07oe5oDzPO99l9ts5NRyArH)\n",
        "![image](https://drive.google.com/uc?id=1ZroBarcnlsr85gibeqEF-MtY13xJTG47)\n",
        "\n",
        "Reorganize the original $4320 \\times 18$ data into 12 sets of 18 (features) $Ã—$ 480 (hours) data according to each month. The image descriptions show two images that cannot be translated without context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBnrGYXu9dZQ"
      },
      "source": [
        "month_data = {}\n",
        "for month in range(12):\n",
        "    sample = np.empty([18, 480])\n",
        "    for day in range(20):\n",
        "        sample[:, day * 24 : (day + 1) * 24] = raw_data[18 * (20 * month + day) : 18 * (20 * month + day + 1), :]\n",
        "    month_data[month] = sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhVmtFEQ9D6t"
      },
      "source": [
        "## Extract Features (2)\n",
        "![alt text](https://drive.google.com/uc?id=1m2cDxtOURX4w3JernurpmuTgwc3sjREB)\n",
        "![alt text](https://drive.google.com/uc?id=1FRWWiXQ-Qh0i9tyx0LiugHYF_xDdkhLN)\n",
        "\n",
        "here are 480 hours per month, with one data point generated every 9 hours. There will be 471 data points per month, resulting in a total of $471 \\times 12$ data points. Each data point has $9 \\times 18$ features (18 features per hour $\\times$ 9 hours).\n",
        "There will be $471 \\times 12$ corresponding targets (PM2.5 at the 10th hour)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcOrC4Fi-n3i"
      },
      "source": [
        "x = np.empty([12 * 471, 18 * 9], dtype = float)\n",
        "y = np.empty([12 * 471, 1], dtype = float)\n",
        "for month in range(12):\n",
        "    for day in range(20):\n",
        "        for hour in range(24):\n",
        "            if day == 19 and hour > 14:  # there must be 9 hours remaining\n",
        "                continue\n",
        "            x[month * 471 + day * 24 + hour, :] = month_data[month][:,day * 24 + hour : day * 24 + hour + 9].reshape(1, -1) #vector dim:18*9 (9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9)\n",
        "            y[month * 471 + day * 24 + hour, 0] = month_data[month][9, day * 24 + hour + 9] #value\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wOii0TX8IwE"
      },
      "source": [
        "## Normalize\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceMqFoNI8ftQ"
      },
      "source": [
        "mean_x = np.mean(x, axis = 0) #18 * 9 \n",
        "std_x = np.std(x, axis = 0) #18 * 9 \n",
        "for i in range(len(x)): #12 * 471\n",
        "    for j in range(len(x[0])): #18 * 9 \n",
        "        if std_x[j] != 0:\n",
        "            x[i][j] = (x[i][j] - mean_x[j]) / std_x[j]\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzvXP5Jya64j"
      },
      "source": [
        "## Split Training Data Into \"train_set\" and \"validation_set\"\n",
        "\n",
        "This section provides a simple demonstration for the second and third questions in the homework report, generating a train_set for training and a validation_set that will not be included in the training process but only used for verification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feF4XXOQb5SC"
      },
      "source": [
        "import math\n",
        "x_train_set = x[: math.floor(len(x) * 0.8), :]\n",
        "y_train_set = y[: math.floor(len(y) * 0.8), :]\n",
        "x_validation = x[math.floor(len(x) * 0.8): , :]\n",
        "y_validation = y[math.floor(len(y) * 0.8): , :]\n",
        "print(x_train_set)\n",
        "print(y_train_set)\n",
        "print(x_validation)\n",
        "print(y_validation)\n",
        "print(len(x_train_set))\n",
        "print(len(y_train_set))\n",
        "print(len(x_validation))\n",
        "print(len(y_validation))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-qAu0KR_ZRR"
      },
      "source": [
        "## Training\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1xIXvqZ4EGgmxrp7c9r0LOVbcvd4d9H4N)\n",
        "![alt text](https://drive.google.com/uc?id=1S42g06ON5oJlV2f9RukxawjbE4NpsaB6)\n",
        "![alt text](https://drive.google.com/uc?id=1BbXu-oPB9EZBHDQ12YCkYqtyAIil3bGj)\n",
        "\n",
        "(Different from the previous figure: the code below uses Root Mean Square Error)\n",
        "\n",
        "Because of the existence of the constant term, an additional column is needed for the dimension (dim); the eps term is added to avoid the denominator of adagrad being 0.\n",
        "\n",
        "Each dimension (dim) corresponds to its own gradient and weight (w), and is learned through iterations (iter_time)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCzDfxBFBFqp"
      },
      "source": [
        "dim = 18 * 9 + 1\n",
        "w = np.zeros([dim, 1])\n",
        "x = np.concatenate((np.ones([12 * 471, 1]), x), axis = 1).astype(float)\n",
        "learning_rate = 100\n",
        "iter_time = 1000\n",
        "adagrad = np.zeros([dim, 1])\n",
        "eps = 0.0000000001\n",
        "for t in range(iter_time):\n",
        "    loss = np.sqrt(np.sum(np.power(np.dot(x, w) - y, 2))/471/12)#rmse\n",
        "    if(t%100==0):\n",
        "        print(str(t) + \":\" + str(loss))\n",
        "    gradient = 2 * np.dot(x.transpose(), np.dot(x, w) - y) #dim*1\n",
        "    adagrad += gradient ** 2\n",
        "    w = w - learning_rate * gradient / np.sqrt(adagrad + eps)\n",
        "np.save('weight.npy', w)\n",
        "w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqNdWKsYBK28"
      },
      "source": [
        "## Testing\n",
        "![alt text](https://drive.google.com/uc?id=1165ETzZyE6HStqKvgR0gKrJwgFLK6-CW)\n",
        "\n",
        "Load the test data and preprocess and feature extract it in a similar way to the training data, so that the test data forms 240 data points with dimensions of 18 * 9 + 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AALygqJFCWOA"
      },
      "source": [
        "# testdata = pd.read_csv('gdrive/My Drive/hw1-regression/test.csv', header = None, encoding = 'big5')\n",
        "testdata = pd.read_csv('./test.csv', header = None, encoding = 'big5')\n",
        "test_data = testdata.iloc[:, 2:]\n",
        "test_data[test_data == 'NR'] = 0\n",
        "test_data = test_data.to_numpy()\n",
        "test_x = np.empty([240, 18*9], dtype = float)\n",
        "for i in range(240):\n",
        "    test_x[i, :] = test_data[18 * i: 18* (i + 1), :].reshape(1, -1)\n",
        "for i in range(len(test_x)):\n",
        "    for j in range(len(test_x[0])):\n",
        "        if std_x[j] != 0:\n",
        "            test_x[i][j] = (test_x[i][j] - mean_x[j]) / std_x[j]\n",
        "test_x = np.concatenate((np.ones([240, 1]), test_x), axis = 1).astype(float)\n",
        "test_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJQks9JEHR6W"
      },
      "source": [
        "## Prediction\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1165ETzZyE6HStqKvgR0gKrJwgFLK6-CW)\n",
        "\n",
        "With the weight and test data, the target can be predicted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNyB229jHsEQ"
      },
      "source": [
        "w = np.load('weight.npy')\n",
        "ans_y = np.dot(test_x, w)\n",
        "ans_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKMKW7RzHwuO"
      },
      "source": [
        "## Save Prediction to CSV File\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwfpqqy0H8en"
      },
      "source": [
        "import csv\n",
        "with open('submit.csv', mode='w', newline='') as submit_file:\n",
        "    csv_writer = csv.writer(submit_file)\n",
        "    header = ['id', 'value']\n",
        "    print(header)\n",
        "    csv_writer.writerow(header)\n",
        "    for i in range(240):\n",
        "        row = ['id_' + str(i), ans_y[i][0]]\n",
        "        csv_writer.writerow(row)\n",
        "        print(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y54yWq9cIPR4"
      },
      "source": [
        "Related reference can refer to:\n",
        "\n",
        "Adagrad:\n",
        "https://youtu.be/yKKNr-QKz2Q?list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&t=705\n",
        "\n",
        "RMSprop:\n",
        "https://www.youtube.com/watch?v=5Yt-obwvMHI\n",
        "\n",
        "Adam\n",
        "https://www.youtube.com/watch?v=JXQT_vxqwIs\n",
        "\n",
        "\n",
        "The above print part is mainly to see the presentation of data and results, and it doesn't hurt to remove it. In addition, in your own Linux system, you can replace the hard-coded part of the file with the use of sys.argv (you can enter the file and file location yourself in the terminal).\n",
        "\n",
        "Finally, you can go beyond the baseline by adjusting the learning rate, iter_time (number of iterations), the number of features used (how many hours to take, which feature fields to take), and even different models."
      ]
    }
  ]
}